{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586165b9",
   "metadata": {},
   "source": [
    "# Tutorial: Ensemble Methods for Electricity Demand Prediction\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.in2p3.fr%2Fenergy4climate%2Fpublic%2Feducation%2Fmachine_learning_for_climate_and_energy/master?filepath=book%2Fnotebooks%2F09_tutorial_ensemble_methods_demand_prediction.ipynb)\n",
    "\n",
    "Tutorial to the class [Ensemble Methods](09_ensemble_methods.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30780159",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tutorial Objectives</b>\n",
    "    \n",
    "- Use ensemble methods to best predict the regional electricity demand in France from a large number of input variables;\n",
    "- Control the complexity parameter(s) of each ensemble method to avoid overfitting;\n",
    "- Compare the skills of different methods.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c7ddc",
   "metadata": {},
   "source": [
    "## Getting ready\n",
    "\n",
    "### Loading the demand data\n",
    "\n",
    "Let us load the regional electricity demand data as in [Tutorial: Supervised Learning Problem and Least Squares](2_tutorial_supervised_learning_problem_ols.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a14a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path manipulation module\n",
    "from pathlib import Path\n",
    "# Numerical analysis module\n",
    "import numpy as np\n",
    "# Formatted numerical analysis modules\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# Plot module\n",
    "import matplotlib.pyplot as plt\n",
    "# Default colors\n",
    "RC_COLORS = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "# Matplotlib configuration\n",
    "plt.rc('font', size=14)\n",
    "\n",
    "# Set data directory\n",
    "data_dir = Path('data')\n",
    "\n",
    "# Set keyword arguments for pd.read_csv\n",
    "kwargs_read_csv = dict(index_col=0, header=0, parse_dates=True)\n",
    "\n",
    "# Set first and last years\n",
    "FIRST_YEAR = 2014\n",
    "LAST_YEAR = 2019\n",
    "\n",
    "# Define electricity demand filepath and label\n",
    "dem_filename = 'reseaux_energies_demand_demand.csv'\n",
    "dem_filepath = Path(data_dir, dem_filename)\n",
    "dem_label = 'Electricity consumption (MWh)'\n",
    "\n",
    "# Read hourly demand data averaged over each region\n",
    "df_dem = pd.read_csv(dem_filepath, **kwargs_read_csv)\n",
    "# region_rename = {\n",
    "#     'Auvergne-Rhône-Alpes': 'Auvergne-Rhône-Alpes',\n",
    "#     'Bourgogne-Franche-Comté': 'Bourgogne-Franche-Comté',\n",
    "#     'Bretagne': 'Bretagne',\n",
    "#     'Centre-Val de Loire': 'Centre-Val de Loire',\n",
    "#     'Grand Est': 'Grand Est',\n",
    "#     'Hauts-de-France': 'Hauts-de-France',\n",
    "#     'Île-de-France': 'Ile-de-France',\n",
    "#     'Normandie': 'Normandie',\n",
    "#     'Nouvelle-Aquitaine': 'Nouvelle-Aquitaine',\n",
    "#     'Occitanie': 'Occitanie',\n",
    "#     'Pays de la Loire': 'Pays-de-la-Loire',\n",
    "#     \"Provence-Alpes-Côte d'Azur\": 'PACA'\n",
    "# }\n",
    "# df_dem.columns = [region_rename[c] for c in df_dem.columns]\n",
    "df_dem.index = df_dem.index.tz_localize(None)\n",
    "df_dem = df_dem.resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f72deb",
   "metadata": {},
   "source": [
    "### Loading the climate data\n",
    "\n",
    "Let us load climate data for France and average it over regions to use it as input to predict the demand.\n",
    "\n",
    "In addition to the surface temperature (in °C), we also load the:\n",
    "- surface density (kg/m3)\n",
    "- surface downward radiation (in W/m2),\n",
    "- surface specific humidity and\n",
    "- surface wind (in m/s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories where you saved the data\n",
    "filename_climate = (\n",
    "    'merra2_area_selection_output_{}_merra2_2010-2019_daily.csv')\n",
    "temp_label = 'Temperature (°C)'\n",
    "\n",
    "def get_regional_climate(variable_name):\n",
    "    filename = filename_climate.format(variable_name)\n",
    "    filepath = Path(data_dir, filename)\n",
    "    df_climate = pd.read_csv(filepath, **kwargs_read_csv)\n",
    "    da_climate = df_climate.to_xarray().to_array('region').transpose(\n",
    "        'time', 'region').to_dataset(name=variable_name)\n",
    "    \n",
    "    return da_climate\n",
    "\n",
    "# Read a climate variable and plot its mean over time\n",
    "ds_clim = get_regional_climate('surface_temperature') - 273.15\n",
    "ds_clim = ds_clim.merge(get_regional_climate('surface_density'))\n",
    "ds_clim = ds_clim.merge(get_regional_climate('surface_downward_radiation'))\n",
    "ds_clim = ds_clim.merge(get_regional_climate('surface_specific_humidity'))\n",
    "ds_clim = ds_clim.merge(get_regional_climate('zonal_wind'))\n",
    "ds_clim = ds_clim.merge(get_regional_climate('meridional_wind'))\n",
    "ds_clim['wind_speed'] = np.sqrt(ds_clim['zonal_wind']**2 +\n",
    "                                ds_clim['meridional_wind']**2)\n",
    "ds_clim['wind_direction'] = np.arctan2(ds_clim['meridional_wind'],\n",
    "                                       ds_clim['zonal_wind'])\n",
    "variable_names = list(ds_clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a9e32",
   "metadata": {},
   "source": [
    "### Preprocessing the inputs\n",
    "\n",
    "We now select data for a given region of France, subsample it to daily frequency and keep only the dates that are common to the demand times series and the climate time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514dc99-e39a-4cb5-a694-9e4375ce4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGION_NAME = 'Ile-de-France'\n",
    "REGION_NAME = 'Occitanie'\n",
    "\n",
    "# Select region\n",
    "df_dem_reg = df_dem[REGION_NAME]\n",
    "ds_clim_reg = ds_clim.sel(region=REGION_NAME, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d6b34-dc14-4bce-9bc0-6fc3c0849d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select common index\n",
    "idx = df_dem_reg.index.intersection(ds_clim_reg.indexes['time'])\n",
    "df_dem_reg = df_dem_reg.loc[idx]\n",
    "ds_clim_reg = ds_clim_reg.sel(time=idx, method='nearest')\n",
    "\n",
    "print('First common date: \\t{}\\nLast common date: \\t{}'.format(\n",
    "    df_dem_reg.index[0], df_dem_reg.index[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be8056-c880-467f-9c76-f2927cea8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to demand (optional)\n",
    "NOISE_LEVEL = 0.\n",
    "df_dem_reg += np.random.normal(0., df_dem_reg.std(), df_dem_reg.shape) * NOISE_LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fdd35-c158-41f0-9e2f-983908b80bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deteriorate dataset by removing extreme temperatures\n",
    "T_MIN = -100.\n",
    "T_MAX = 100.\n",
    "extremes = (ds_clim_reg['surface_temperature'] < T_MIN) | (ds_clim_reg['surface_temperature'] > T_MAX)\n",
    "ds_clim_reg = ds_clim_reg.where(~extremes).dropna(dim='time')\n",
    "df_dem_reg = df_dem_reg.where(~extremes).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf703bd-245d-4da2-8c0e-02157615692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deteriorate by selecting a few days per month\n",
    "N_DAYS_PER_MONTH = 1\n",
    "step = int(365.25 / 12 / N_DAYS_PER_MONTH)\n",
    "ds_clim_reg = ds_clim_reg.isel(time=slice(None, None, step))\n",
    "df_dem_reg = df_dem_reg.iloc[slice(None, None, step)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ed0fb",
   "metadata": {},
   "source": [
    "### Analyzing the relationship between the climate variables and the demand\n",
    "\n",
    "The code below:\n",
    "- does a scatter plot the demand as a function of each climate variable on separate figures,\n",
    "- computes the correlation between the demand and each climate variable,\n",
    "- computes the correlation matrix between climate variables removing values smaller than 0.3 in absolute value.\n",
    "\n",
    "> ***Question***\n",
    "> - Does their seem to be redundancies between climate variables?\n",
    "> - Which climate variables seem to be most relevant to predict the demand?\n",
    "> - Discuss the limits of this analysis using correlations alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable_name, da in ds_clim_reg.items():\n",
    "    plt.figure()\n",
    "    plt.scatter(da.values, df_dem_reg.values, s=10, alpha=0.2)\n",
    "    plt.xlabel(da.name)\n",
    "    plt.ylabel(dem_label)\n",
    "    c = np.corrcoef(da.values, df_dem_reg.values)[0, 1]\n",
    "    plt.title('{}, Correlation = {:.2f}'.format(variable_name, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b29c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_clim = np.corrcoef(ds_clim_reg.to_array().values)\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(np.where(np.abs(corr_clim) > 0.3, corr_clim, np.nan),\n",
    "           vmin=-1., vmax=1.)\n",
    "ticks = range(len(variable_names))\n",
    "ticklabels = variable_names\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(ticklabels, rotation=90.)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_yticklabels(ticklabels)\n",
    "plt.colorbar()\n",
    "_ = plt.title('Correlation matrix (abs. val. > 0.3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be099a",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1cb82",
   "metadata": {},
   "source": [
    "### Preparing feature extraction\n",
    "\n",
    "As in the previous tutorials, we define nonlinear features from the temperature based on heating- and cooling-temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ad35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define heating and cooling temperature thresholds\n",
    "TEMP_HEAT = 15.\n",
    "TEMP_COOL = 20.\n",
    "\n",
    "# Define function returning a dictionary from variable name\n",
    "# to variable train data with base, heating and cooling variables\n",
    "def get_heat_cool(x):\n",
    "    return {\n",
    "        'heat': (TEMP_HEAT - x) * (x < TEMP_HEAT).astype(float),\n",
    "        'cool': (x - TEMP_COOL) * (x > TEMP_COOL).astype(float)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f6c44",
   "metadata": {},
   "source": [
    "We also define a factorization by month.\n",
    "In other words, for each variable, we define 12 new variables for which the values are that of the original variable for dates in the corresponding month, zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_monthly(variables, index):\n",
    "    new_variables = {}\n",
    "    for variable_name, variable_data in variables.items():\n",
    "        for month in range(1, 13):\n",
    "            new_variable_name = '{}_m{:02d}'.format(variable_name, month)\n",
    "            new_variables[new_variable_name] = variable_data * (\n",
    "                index.month == month).astype(float)\n",
    "            \n",
    "    return new_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3dc4e7-848b-4ba2-b977-0dfb97e2dafe",
   "metadata": {},
   "source": [
    "> ***Question***\n",
    "> - If 4 climate variables are selected, how may features will result from the monthly factorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251a223-6eab-43c7-ab22-2f99326e5168",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27372d8",
   "metadata": {},
   "source": [
    "### Regressions evaluation function\n",
    "\n",
    "We also define a function to evaluate any of our regressions.\n",
    "\n",
    "`FIRST_TEST_YEAR` controls the number of years at the end of the time series to keep as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c54241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Default number of test days\n",
    "FIRST_TEST_YEAR = 2018\n",
    "\n",
    "TIME = ds_clim_reg.indexes['time']\n",
    "def evaluate_regressor(\n",
    "    reg, reg_kwargs, param_name, param_range, first_test_year=FIRST_TEST_YEAR,\n",
    "    n_splits=None, cv_iterator=model_selection.GroupKFold,\n",
    "    plot_validation=True):\n",
    "    \n",
    "    if n_splits is None:\n",
    "        n_splits = len(np.unique(TIME[TIME.year < first_test_year].year))\n",
    "    \n",
    "    # Get test data keeping last years\n",
    "    index_test = TIME.year >= first_test_year\n",
    "    x_test = x[index_test]\n",
    "    X_test = X[index_test]\n",
    "    y_test = y[index_test]\n",
    "    \n",
    "    # Select train data from first years and first days in month \n",
    "    index_train = TIME.year < first_test_year\n",
    "    X_train = X[index_train]\n",
    "    y_train = y[index_train]\n",
    "    \n",
    "    if plot_validation:\n",
    "        # Set cross-validation iterator\n",
    "        cv = cv_iterator(n_splits=n_splits)\n",
    "        groups = TIME[index_train].year\n",
    "\n",
    "        # Get train and validation scores from cross-validation\n",
    "        train_scores, validation_scores = model_selection.validation_curve(\n",
    "            reg, X_train, y_train, param_name=param_name,\n",
    "            param_range=param_range, cv=cv, groups=groups)\n",
    "\n",
    "        # Get train curve\n",
    "        train_scores_mean = train_scores.mean(1)\n",
    "        train_scores_max = train_scores.max(1)\n",
    "        train_scores_min = train_scores.min(1)\n",
    "\n",
    "        # Get validation curve\n",
    "        validation_scores_mean = validation_scores.mean(1)\n",
    "        validation_scores_max = validation_scores.max(1)\n",
    "        validation_scores_min = validation_scores.min(1)\n",
    "\n",
    "        # Get best value of the regularization parameter\n",
    "        i_best = np.argmax(validation_scores_mean)\n",
    "        param_best = param_range[i_best]\n",
    "        score_best = validation_scores_mean[i_best]\n",
    "\n",
    "    \n",
    "        # Plot validation curve\n",
    "        lw = 2\n",
    "        plt.figure()\n",
    "        plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "        plt.fill_between(param_range, train_scores_min, train_scores_max,\n",
    "                         alpha=0.2, color=\"darkorange\", lw=lw)\n",
    "        plt.semilogx(param_range, validation_scores_mean,\n",
    "                     label=\"Cross-validation score\", color=\"navy\", lw=lw)\n",
    "        plt.fill_between(\n",
    "            param_range, validation_scores_min, validation_scores_max,\n",
    "            alpha=0.2, color=\"navy\", lw=lw)\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel(r'Validation $R^2$')\n",
    "        plt.title(REGION_NAME + r'. Best $R^2$: {:.2} for {} = {:.1e}'.format(\n",
    "            score_best, param_name, param_best))\n",
    "        plt.ylim(0.0, 1.1)\n",
    "        plt.legend(loc=\"best\")\n",
    "        \n",
    "        # Set best parameter\n",
    "        reg.set_params(**{param_name: param_best})\n",
    "    else:\n",
    "        param_best = reg.get_params(param_name)\n",
    "        \n",
    "    # Compute prediction error conditioned on first 5 years of data\n",
    "    reg.fit(X_train, y_train)\n",
    "    test_score = reg.score(X_test, y_test)\n",
    "    print('\\nTest R2: {:.2f}'.format(test_score))\n",
    "\n",
    "    # Predict for work days and off days\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    # Plot prediction on top of train data versus temperature\n",
    "    plt.figure()\n",
    "    plt.scatter(x_test, y_test, s=10, alpha=0.5)\n",
    "    plt.scatter(x_test, y_pred, s=10)\n",
    "    plt.xlabel(temp_label)\n",
    "    plt.ylabel(dem_label)\n",
    "    plt.title(REGION_NAME)\n",
    "\n",
    "    # Scatter plot of prediction against target\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, y_pred, s=10, alpha=0.5)\n",
    "    plt.xlabel('Target ' + dem_label)\n",
    "    plt.ylabel('Predicted ' + dem_label)\n",
    "    plt.title(REGION_NAME)\n",
    "    \n",
    "    return {param_name: param_best}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920360a",
   "metadata": {},
   "source": [
    "> ***Question***\n",
    "> - If 2018 is given as the first test years, how many training years and test years will be available for this dataset?\n",
    "> - Identify the lines where:\n",
    ">   - The validation and train scores are computed;\n",
    ">   - The test score is computed;\n",
    ">   - The target is predicted from the test input features.\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d136c99",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "The following code select features, by controlling:\n",
    "- The climate variables to select in addition to the heating- and cooling-temperatures;\n",
    "- Whether a monthly factorization is performed or not;\n",
    "- The degree of polynomial interactions;\n",
    "- The standardization of the features.\n",
    "\n",
    "It also plots the time series features associated with each selected climate variable.\n",
    "\n",
    "> ***Question***\n",
    "> - Identify the lines where:\n",
    ">   - The heating and cooling temperatures are obtained;\n",
    ">   - The other climate variables are added;\n",
    ">   - The monthly factorization is applied;\n",
    ">   - The polynomial features are computed;\n",
    ">   - The standardization is performed.\n",
    "> - Are the feature time series in agreement with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Variables in addition to temperature features\n",
    "ADDITIONAL_VARIABLE_NAMES = []\n",
    "# ADDITIONAL_VARIABLE_NAMES = [\n",
    "#     'surface_density', 'surface_downward_radiation',\n",
    "#     'surface_specific_humidity', 'wind_speed', 'wind_direction'\n",
    "# ]\n",
    "\n",
    "# Degree of polynomial interactions\n",
    "POLY_DEGREE = 1\n",
    "\n",
    "# Get features\n",
    "x = ds_clim_reg['surface_temperature'].values\n",
    "y = df_dem_reg.values\n",
    "features = get_heat_cool(x)\n",
    "for var_name in ADDITIONAL_VARIABLE_NAMES:\n",
    "    features[var_name] = ds_clim_reg[var_name].values\n",
    "feature_names_nofact = list(features)\n",
    "\n",
    "# Factorize monthly\n",
    "features = factorize_monthly(features, TIME)\n",
    "\n",
    "# Plot features before polynomial transform and standardization\n",
    "feature_names = list(features)\n",
    "df_features = pd.DataFrame(features, index=TIME)\n",
    "n_feat_nofact = len(feature_names_nofact)\n",
    "fig, axs = plt.subplots(n_feat_nofact, figsize=[12, 5 * n_feat_nofact])\n",
    "for k, feature_name in enumerate(feature_names_nofact):\n",
    "    try:\n",
    "        ax = axs[k]\n",
    "    except TypeError:\n",
    "        ax = axs\n",
    "    feat_variables = [feature_name in v for v in feature_names]\n",
    "    df_features.iloc[:, feat_variables].plot(ax=ax, ylabel=feature_name)\n",
    "    ax.legend(bbox_to_anchor=(1,1), loc='upper left', title='Features')\n",
    "\n",
    "# Get input matrix\n",
    "X = np.array(list(features.values())).T\n",
    "if POLY_DEGREE > 1:\n",
    "    poly = preprocessing.PolynomialFeatures(POLY_DEGREE)\n",
    "    X =  poly.fit_transform(X)\n",
    "    \n",
    "# Standardize\n",
    "X = preprocessing.StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73517e4c",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbec3c",
   "metadata": {},
   "source": [
    "## Individual models\n",
    "\n",
    "First, make sure that only the heating and cooling temperatures are selected as features, that the monthly factorization is activated and that no polynomial transformation is performed.\n",
    "\n",
    "### Lasso regression\n",
    "\n",
    "The following code:\n",
    "- Creates a Lasso regressor with positive coefficients;\n",
    "- Evaluate the regressor over a range of regularization parameter values;\n",
    "- Represent the importance of the coefficients.\n",
    "\n",
    "The evaluation function:\n",
    "- Plots the training and validation curves with the lines representing the mean score and the shading the minimum and maximum scores;\n",
    "- Plots test predictions against the test inputs over the train data;\n",
    "- Plots the test predictions against the test targets.\n",
    "\n",
    "> ***Question***\n",
    "> - Is there an overfit/underfit tradeoff?\n",
    "> - Explain the difference between the training curve and the validation curve?\n",
    "> - Explain the evolution the variability of the validation curve.\n",
    "> - Is the test score in agreement with the best validation score?\n",
    "> - How does the the importance of the coefficients vary with climate variable and the month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566445d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Define array of complexity coordinate, regressor and options\n",
    "# for Lasso regression\n",
    "param_name, param_range = 'alpha', np.logspace(-4, 5, 10)\n",
    "reg_kwargs_lasso = dict(fit_intercept=True, warm_start=True,\n",
    "                        positive=True, max_iter=10000)\n",
    "reg_lasso = linear_model.Lasso(\n",
    "    **{param_name: param_range[0]}, **reg_kwargs_lasso)\n",
    "\n",
    "# Evaluate regressor\n",
    "param_best_lasso = evaluate_regressor(\n",
    "    reg_lasso, reg_kwargs_lasso, param_name, param_range)\n",
    "index = feature_names if POLY_DEGREE == 1 else None\n",
    "df_coef = pd.Series(reg_lasso.coef_, index=index)\n",
    "plt.figure()\n",
    "_ = df_coef.plot(kind='barh', figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced7728",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fa822",
   "metadata": {},
   "source": [
    "### Decision-tree regression\n",
    "\n",
    "The following code:\n",
    "- Creates a decision-tree regressor;\n",
    "- Evaluate the regressor over a range of maximum tree depth;\n",
    "\n",
    "> ***Question***\n",
    "> - Is there an overfit/underfit tradeoff?\n",
    "> - How does the tree perform compared to the Lasso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483022e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Define array of complexity coordinate, regressor and options\n",
    "# for decision-tree regression\n",
    "param_name, param_range = 'max_depth', np.arange(1, 150, 1)\n",
    "reg_kwargs_dt = dict()\n",
    "reg_dt = tree.DecisionTreeRegressor(\n",
    "    **{param_name: param_range[0]}, **reg_kwargs_dt)\n",
    "\n",
    "# Evaluate regressor\n",
    "param_best_dt = evaluate_regressor(\n",
    "    reg_dt, reg_kwargs_dt, param_name, param_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f29f55",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a1b7a",
   "metadata": {},
   "source": [
    "## Ensemble models\n",
    "\n",
    "### Bagging regressor\n",
    "\n",
    "The following code:\n",
    "- Creates a bagging regressor with a decision tree as a base estimator;\n",
    "- Evaluate the regressor over a range of number of estimators;\n",
    "\n",
    "> ***Question***\n",
    "> - Use the Scikit-learn documentation to give the parameters defining this bagging regressor.\n",
    "\n",
    "> ***Question***\n",
    "> - Is there an overfit/underfit tradeoff?\n",
    "> - In this case, how to choose the number of estimators?\n",
    "> - Same question without using validation curves.\n",
    "> - How does the bagging perform compared to the individual regressors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# Define array of complexity coordinate, regressor and options\n",
    "# for Bagging regression\n",
    "param_name, param_range = 'n_estimators', np.arange(1, 50, 2)\n",
    "base_estimator = None\n",
    "reg_kwargs_br = dict(base_estimator=base_estimator)\n",
    "reg_br = ensemble.BaggingRegressor(\n",
    "    **{param_name: param_range[0]}, **reg_kwargs_br)\n",
    "\n",
    "# Evaluate regressor\n",
    "param_best_br = evaluate_regressor(\n",
    "    reg_br, reg_kwargs_br, param_name, param_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7038d",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb17c3",
   "metadata": {},
   "source": [
    "### Random-forest regressor\n",
    "\n",
    "The following code:\n",
    "- Creates a random-forest regressor;\n",
    "- Evaluate the regressor over a range of number of estimators;\n",
    "\n",
    "> ***Question***\n",
    "> - Use the Scikit-learn documentation to give the parameters defining this random-forest regressor.\n",
    "\n",
    "> ***Question***\n",
    "> - How does the random forest perform compared to the bagging regressor?\n",
    "> - Compare the varibility of the validation score of the random forest to that of the bagging regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array of complexity coordinate, regressor and options\n",
    "# for random-forest regression\n",
    "param_name, param_range = 'n_estimators', np.arange(1, 202, 25)\n",
    "reg_kwargs_rf = dict()\n",
    "reg_rf = ensemble.RandomForestRegressor(\n",
    "    **{param_name: param_range[0]}, **reg_kwargs_rf)\n",
    "\n",
    "# Evaluate regressor\n",
    "param_best_rf = evaluate_regressor(\n",
    "    reg_rf, reg_kwargs_rf, param_name, param_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f5354",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a46464",
   "metadata": {},
   "source": [
    "The following plot represents the mean and standard deviation of the importance given to the features by the trees in the random forest (see [Feature importance with a forest of trees in Scikit-learn User guide](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#feature-importances-with-a-forest-of-trees)).\n",
    "\n",
    "> ***Question***\n",
    "> - How does the the importance of the features vary with climate variable and the month?\n",
    "> - Compare the importance of the random-forest features with that of the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f77f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance\n",
    "importances = reg_rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in reg_rf.estimators_], axis=0)\n",
    "forest_importances = pd.Series(importances, index=index)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "_ = ax.set_ylabel(\"Mean decrease in impurity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba0266",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911d926",
   "metadata": {},
   "source": [
    "### Voting regressor\n",
    "\n",
    "The following code creates a voting regressor with the Lasso, the decision tree and the random forest as base estimators and tests it.\n",
    "\n",
    "> ***Question***\n",
    "> - Use the Scikit-learn documentation to give the parameters defining this voting regressor.\n",
    "\n",
    "> ***Question***\n",
    "> - How does the voting regressor perform compared to the base estimators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e563e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array of complexity coordinate, regressor and options\n",
    "# for voting regression\n",
    "param_name, param_range = 'estimators', [\n",
    "    [('Lasso', reg_lasso), ('DecisionTree', reg_dt), ('RandomForest', reg_rf)]\n",
    "]\n",
    "reg_kwargs_vr = dict()\n",
    "reg_vr = ensemble.VotingRegressor(\n",
    "    **{param_name: param_range[0]}, **reg_kwargs_vr)\n",
    "\n",
    "# Evaluate regressor\n",
    "param_best_vr = evaluate_regressor(\n",
    "    reg_vr, reg_kwargs_vr, param_name, param_range,\n",
    "    plot_validation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c317e88",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491ff1a",
   "metadata": {},
   "source": [
    "### Stacking regressor\n",
    "\n",
    "- The following code creates a stacking regressor with the Lasso, the decision tree and the random forest as base estimators and tests it.\n",
    "- The final regressor is a OLS with positive coefficients.\n",
    "- The weights given to the base estimators by the stacking are also given.\n",
    "\n",
    "> ***Question***\n",
    "> - Use the Scikit-learn documentation to give the parameters defining this stacking regressor.\n",
    "\n",
    "> ***Question***\n",
    "> - How does the stacking regressor perform compared to the voting regressor? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array of complexity coordinate, regressor and options\n",
    "# for voting regression\n",
    "param_name, param_range = 'estimators', [\n",
    "    [('Lasso', reg_lasso), ('DecisionTree', reg_dt), ('RandomForest', reg_rf)]\n",
    "]\n",
    "reg_kwargs_sr = dict(final_estimator=linear_model.LinearRegression(\n",
    "    positive=True))\n",
    "reg_sr = ensemble.StackingRegressor(\n",
    "    **{param_name: param_range[0]}, **reg_kwargs_sr)\n",
    "\n",
    "# Evaluate regressor\n",
    "param_best_sr = evaluate_regressor(\n",
    "    reg_sr, reg_kwargs_sr, param_name, param_range, plot_validation=False)\n",
    "weights = reg_sr.final_estimator_.coef_\n",
    "index = [r[0] for r in param_range[0]]\n",
    "df_weights = pd.Series(weights, index=index)\n",
    "print('Weights:')\n",
    "print(df_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca367cf8",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e9cda",
   "metadata": {},
   "source": [
    "### AdaBoost regressor\n",
    "\n",
    "The following code:\n",
    "- Creates an AdaBoost regressor;\n",
    "- Evaluate the regressor over a range of number of estimators;\n",
    "\n",
    "> ***Question***\n",
    "> - Use the Scikit-learn documentation to give the parameters defining this AdaBoost regressor.\n",
    "\n",
    "> ***Question***\n",
    "> - How does the AdaBoost compared to the other regressors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define array of complexity coordinate, regressor and options\n",
    "# for AdaBoost regression\n",
    "param_name, param_range = 'n_estimators', np.arange(1, 50, 2)\n",
    "base_estimator = linear_model.LinearRegression(fit_intercept=True)\n",
    "reg_kwargs_abr = dict(base_estimator=base_estimator)\n",
    "reg_abr = ensemble.AdaBoostRegressor(\n",
    "    **{param_name: param_range[0]}, **reg_kwargs_abr)\n",
    "\n",
    "# Evaluate regressor\n",
    "param_best_abr = evaluate_regressor(\n",
    "    reg_abr, reg_kwargs_abr, param_name, param_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43fa30",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4771f",
   "metadata": {},
   "source": [
    "> ***Question (optional)***\n",
    "> - Reevaluate your results when changing:\n",
    ">   - The combination of selected climate variables;\n",
    ">   - The degree of the polynomial features;\n",
    ">   - The activation of the monthly factorization.\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7ee4b",
   "metadata": {},
   "source": [
    "> ***Question (Optional)***\n",
    "> - Reevaluate your results for different regions.\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f039c7",
   "metadata": {},
   "source": [
    "> ***Question (Optional)***\n",
    "> - Reevaluate your results for a different scoring metric.\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed55f4",
   "metadata": {},
   "source": [
    "***\n",
    "## Credit\n",
    "\n",
    "[//]: # \"This notebook is part of [E4C Interdisciplinary Center - Education](https://gitlab.in2p3.fr/energy4climate/public/education).\"\n",
    "Contributors include Bruno Deremble and Alexis Tantet.\n",
    "Several slides and images are taken from the very good [Scikit-learn course](https://inria.github.io/scikit-learn-mooc/).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: flex; height: 70px\">\n",
    "    \n",
    "<img alt=\"Logo LMD\" src=\"images/logos/logo_lmd.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo IPSL\" src=\"images/logos/logo_ipsl.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo E4C\" src=\"images/logos/logo_e4c_final.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo EP\" src=\"images/logos/logo_ep.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo SU\" src=\"images/logos/logo_su.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo ENS\" src=\"images/logos/logo_ens.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo CNRS\" src=\"images/logos/logo_cnrs.png\" style=\"display: inline-block\"/>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; margin-right: 10px\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
    "    <br>This work is licensed under a &nbsp; <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
