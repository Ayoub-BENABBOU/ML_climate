{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb8c31b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification I: Generative models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a485dc1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Prerequisites</b>\n",
    "    \n",
    "- Bayes Theorem\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790b2ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Learning Outcomes</b>\n",
    "    \n",
    "- Bayes decision boundary\n",
    "- Linear discriminant analysis\n",
    "- Naive Bayses classifier\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed73093",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1dc61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Weather icons\n",
    "\n",
    "We have already derived methods to predict quantitative variables (variables that vary continuously). In many problems however, the variable we are trying to predict is qualitative (takes discrete values). It is actually often the case that a complex meteorological situation is summarized by a pictogram or by a warning (Cat 5. hurricane, storm warning, etc.). These categorical variable help establishing an appropriate response plan based on the forecast. To illustrate the transition between quantitative and qualitative variables, we plot below the weather map for a given day and the corresponding weather icons that one can use to produce a weather report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58eb24",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img alt=\"weather\" src=\"images/weather.png\" style=\"float:left\">\n",
    "<img alt=\"Weather icons\" src=\"images/weather-icons.jpg\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf19950",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prediction of the rain\n",
    "\n",
    "Prediction of the rain remains one of the most challenging task in numerical weather prediction. In fact the rain is the result of multiple scale phenomena: from the large-scale organization of weather system to the small scale microphysics of dropplet formation. Getting the right prediction for the rain implies that we have a model that captures well all these scales.\n",
    "\n",
    "Despite the fact that rain is hard to predict, there seem to be exist a correspondance between the surface pressure and the weather conditions as shown in the picture below:\n",
    "\n",
    "<img alt=\"Barometer\" src=\"images/barometer.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0db74b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Seasons\n",
    "\n",
    "As a last example of categorical variables, we use seasons to denote a particular moment of the year and we know that each season is associated to a particular type of weather. In this notebook, we will illustrate the concept of classification with this simple example: try to assess how much a season differ from another.\n",
    "\n",
    "<img alt=\"Seasons\" src=\"images/seasons.jpg\">\n",
    "credits: getty images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2a075",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context: Bayes Decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbb5c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us consider a generic problem where $X$ is a multivariate random vector that describe the state of the atmosphere. For illustrative purposes, we will only consider the case where $X$ is a 2-dimensional vector (e.g. pressure and temperature). We want to classify the current weather into a category $C$. For simplicity will consider only two categories: $k=1$ when it is raining and $k=0$ when it is not raining. \n",
    "\n",
    "Notation: $C$ is the random variable, $k\\in\\{0,1\\}$ is a realization of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7723a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both $X$ and $C$ are random variables except $X$ is a continuous random variable and $C$ is discrete. Hence there exist a probability distribution function \n",
    "\n",
    "\\begin{equation}\n",
    "f_X(\\mathbf x)\n",
    "\\end{equation}\n",
    "\n",
    "that describes how likely it is to observe weather $\\mathbf x$. There also exists a probability\n",
    "\n",
    "\\begin{equation}\n",
    "P(C=k)\n",
    "\\end{equation}\n",
    "\n",
    "to observe either event $c=k$ and $c=k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad0c86",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a given state of the atmosphere $\\mathbf x^{\\rm obs}$, our goal is to evaluate the probability of event $C$. This can be written in terms of conditional probabilities\n",
    "\n",
    "\\begin{equation}\n",
    "P(C=k|X=\\mathbf x^{\\rm obs})\n",
    "\\end{equation}\n",
    "\n",
    "Once this probability is known for all classes, we can assign a class to our observation $\\mathbf x^{\\rm obs}$ which corresponds to the highest probability. For now, we keep the subscript $^{\\rm obs}$ in order to emphasize that this variable is fixed and what we want to evaluate is the probability of assigning a class to that specific observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfece5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Bayes decision boundary* correspond to the points $\\mathbf x$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "P(C=1|X=\\mathbf x) = P(C=2|X=\\mathbf x)\n",
    "\\end{equation}\n",
    "\n",
    "It corresponds to the part of the domain for which the probability of belonging to each class is equal. Our goal is to find this separation boundary and we will mostly describe methods for which the separation is a *linear* function of $\\mathbf x$. All these methods are called *linear classifiers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b8eb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context: Bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d02ae5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sometimes, the direct conditional probability $P(C=k|X=\\mathbf x^{\\rm obs})$ is hard to evaluate (sparse data). However, we can use Bayes theorem that states that\n",
    "\n",
    "\\begin{equation}\n",
    "P(C=k|X=\\mathbf x^{\\rm obs}) = \\frac{f_{X|C}(\\mathbf x^{\\rm obs} | k) P(C=k)}{f_X(\\mathbf x^{\\rm obs})}\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "- $f_{X|C}(\\mathbf x^{\\rm obs} | k)$ is called the *likelihood* of observing event $C$ given the observation $\\mathbf x^{\\rm obs}$\n",
    "- $P(C=k)$ is the *prior* observation of observing event $C$\n",
    "- $f_X(\\mathbf x^{\\rm obs})$ is just a normalization factor also called the marginal probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2813a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Likelihood\n",
    "\n",
    "> ***Question***\n",
    "> - What are the two conditions required for $f$ to be a probability distribution function.\n",
    "\n",
    "In general $f_{X|C}(\\mathbf x | k)$ is a probability (density) of observing $\\mathbf x$ given $C=k$. In this context where $k$ is fixed, it is indeed a probability because $f$ satisfies the following two conditions cited above.\n",
    "\n",
    "\n",
    "However, if we now consider the case $\\mathbf x$ is given and $c$ is variable, then $f_{X|C}(\\mathbf x | k)$ is no longer a probability: it is a *likelihood*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda13b1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - Make sure you understand the concept of likelihood if your observation of the atmosphere is that the temperature is 3 degrees and you want to know the likelihood of the *rain* event. Does $f_{X|C}(3~degC| rain)$ and  $f_{X|C}(3~degC| no\\_rain)$ sum up to 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fada9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prior\n",
    "\n",
    "The prior $P(C=k)$, as its name indicates, is the probability of observing $c$ without any other information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23c210",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - Can you write the marginal probability $f_X(\\mathbf x)$ as a function of $f_{X|C}(\\mathbf x | k)$ and $P(C=k)$ to show that it is indeed a renormalization factor in the Bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b8956",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549dab38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The field of *Generative models* consists in estimating the *probability* \n",
    "\n",
    "\\begin{equation}\n",
    "f_{X|C}(\\mathbf x | k)\n",
    "\\end{equation}\n",
    "\n",
    "for all $k$ and all $\\mathbf x$. Once we have a model for this probability, we can use Bayes' theorem to infer the quantity of interest, namely \n",
    "\n",
    "\\begin{equation}\n",
    "P(C=k|X=\\mathbf x^{\\rm obs})\\, .\n",
    "\\end{equation}\n",
    "\n",
    "We recall that depending on the context, $f_{X|C}(\\mathbf x | k)$ is either a likelihood (variable $\\mathbf x$, fixed $k$) or a probability (fixed $\\mathbf x$, variable $k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3585cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16664d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Case with 1 predictor (p=1)\n",
    "\n",
    "To illustrate the idea of linear discriminant analysis, we will first consider the case where there is only 1 predictor, that is $\\mathbf x$ is a scalar (that we thus write $x$). The number of classes is $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e025363",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One proposition for $f_{X|C}(x | k)$ is to suppose that for each class, the data is distributed according to the *Gaussian distribution*\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat f_{X|C}(x | k) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(  -\\frac{1}{2} \\frac{(x-\\mu_k)^2}{\\sigma^2}\\right)\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "that is $\\mathcal N (\\mu_k,\\sigma^2)$ where the mean $\\mu_k$ is specific to each class, but the covariance is common to all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f1cbe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose that we have $N_k$ observations in each class (with a total of $N$ observations). We gather all the observations that belong to a class $k$ to compute the mean and variance of the class. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\mu_k = \\frac{1}{N_k} \\sum_{i\\in C_k} x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\sigma_k = \\frac{1}{N_k-1} \\sum_{i\\in C_k} (x_i - \\hat \\mu_k)^2\n",
    "\\end{equation}\n",
    "\n",
    "where the sum spans only the observations that belong to the class $k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2ad5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weighted average of the variances is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\sigma = \\frac{1}{N-K} \\sum_{k} (N_k-1) \\hat \\sigma_k \n",
    "\\end{equation}\n",
    "\n",
    "So *on average* each observation belongs to a distribution of mean $\\hat \\mu_k$ and variance $\\hat \\sigma$.\n",
    "\n",
    "We can use these values in our expression of the Gaussian distribution above in order to fully characterize $f_{X|C}(x | k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe196ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that we are interested in $P(C=k|X=\\mathbf x^{\\rm obs})$, and in order to use the Bayes Theorem, we still need to estimate the prior $P(C=k)$.\n",
    "\n",
    "> ***Question***\n",
    "> - Given the sample that we have used so far, what is $\\hat P_k$ the most obvious estimate of $P(C=k)$, the probability that an observation falls into the class $k$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bdf0d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now gather all bits and pieces to construct the final probability of observation $x$ of belonging to class $k$\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat P(C=k|X=x ) = \\frac{\\hat P_k \\hat f_{X|C}(x | k) }{\\sum_i  \\hat P_i \\hat f_{X|C}(x | i)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a39f27",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a given observation, we can assign the class $k$ that has the largest probability. Since the log is a monotonic function, we can also compare the log of the probability and get the same result. If we do that, we can then assign an observation to the class for which \n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_k (x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(P_k)\n",
    "\\end{equation}\n",
    "\n",
    "is the highest. This quantity is called the discriminant function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7aa0d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We get that the discriminant is a *linear* function of $x$. It is linear because each pdf $f_{X|C}(x | k)$ has the same variance for all classes. This causes the non linear term to cancel in the expression of the discriminant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92dab4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The advantage of LDA is that there are less parameters to adjust (less prone to overfitting). and the obvious drawback is that each class do not necessarily obey the same distribution. If we relax that approximation, we get a decision boundary which is no longer linear: it is called *Quadratic discriminant analysis* (QDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53890d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question (optional)***\n",
    "> - Derive the equation of the linear discriminant function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e58de6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Generalization of LDA to  $p>1$\n",
    "\n",
    "All the formalism holds in higher dimensions: the likelihood in a $p$-dimensional space is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat f_{X|C}(\\mathbf x | k) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}} \\exp \\left(  -\\frac{1}{2} (\\mathbf x-\\mathbf \\mu_k)^\\top \\Sigma^{-1}(\\mathbf x-\\mathbf \\mu_k)\\right)\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Sigma$ is the covariance matrix and $|\\Sigma|$ is the determinant of that matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf909b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the discriminant function is\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_k (x) = \\mathbf x^\\top \\Sigma^{-1} \\mathbf \\mu_k - \\frac{1}{2}\\mathbf \\mu_k^\\top \\Sigma^{-1} \\mathbf \\mu_k  + \\log(P_k)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49329d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>LDA summary</b>\n",
    "    \n",
    "- Use Bayes to inverse the problem (Generative model)\n",
    "- Decision boundary is linear\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad67f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb183cac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Naive Bayes classifier is a popular method that share many similarities with LDA. Like LDA, it is a generative model or which we try to predict $f(\\mathbf x |k)$.\n",
    "\n",
    "In order to estimate that probability, the naive Bayes methods assume *naively* that all variables are independent so\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\mathbf x |k) = f(x_1 |k)f(x_2 |k)...f(x_p |k)\n",
    "\\end{equation}\n",
    "\n",
    "and also\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\mathbf x) = f(x_1)f(x_2)\n",
    "\\end{equation}\n",
    "\n",
    "which we will needed for the normalization in the Bayes theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fcb599",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This hypothesis of independent variables is a very strong hypothesis so you should not use this method when you know this is not true. Classification of images for instance will often fail because nearby pixels are correlated.\n",
    "\n",
    "\n",
    "\n",
    "This method is still attractive because is is much simpler to estimate a 1-dimensional pdf  rather than a $p$-dimensional pdf. This method will also be useful when the number of features $p$ is bigger than the number of observations $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00851f30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e186998",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "## Credit\n",
    "\n",
    "[//]: # \"This notebook is part of [E4C Interdisciplinary Center - Education](https://gitlab.in2p3.fr/energy4climate/public/education).\"\n",
    "Contributors include Bruno Deremble and Alexis Tantet.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: flex; height: 70px\">\n",
    "    \n",
    "<img alt=\"Logo LMD\" src=\"images/logos/logo_lmd.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo IPSL\" src=\"images/logos/logo_ipsl.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo E4C\" src=\"images/logos/logo_e4c_final.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo EP\" src=\"images/logos/logo_ep.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo SU\" src=\"images/logos/logo_su.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo ENS\" src=\"images/logos/logo_ens.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo CNRS\" src=\"images/logos/logo_cnrs.png\" style=\"display: inline-block\"/>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; margin-right: 10px\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
    "    <br>This work is licensed under a &nbsp; <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
